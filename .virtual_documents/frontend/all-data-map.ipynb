import requests
import pandas as pd
import json

from elasticsearch import Elasticsearch





# Connect to Elasticsearch
es = Elasticsearch("http://localhost:9200")

# Get list of all indices
indices = es.indices.get_alias("*")
for index in indices:
    print(index)





# Define the API endpoints
endpoints = [
    "http://127.0.0.1:9200/sudo/building-approvals",
     "http://127.0.0.1:9200/sudo/economy-and-industry",
    "http://127.0.0.1:9200/sudo/medians-and-averages"
]

# Fetch data from each endpoint and print the first response
for endpoint in endpoints:
    response = requests.get(endpoint)
    if response.status_code == 200:
        print("status code: 200")
        data = response.json()
        if data:
            # Print the first response in an organized JSON format
            print(json.dumps(data[0], indent=4))
        else:
            print(f"No data returned from {endpoint}")
    else:
        print(f"Failed to fetch data from {endpoint}")



def standardize_keys(data):
    standardized_data = []
    for item in data:
        standardized_item = {
            "year": item.get("yr") or item.get(" yr"),
            "sa4_code": item.get(" sa4_code") or item.get(" sa4_code_2016") or item.get(" SA4_MAIN11"),
            "sa4_name": item.get("sa4_name") or item.get(" sa4_name") or item.get("sa4_name_2016") or item.get(" SA4_NAME11"),
            "attached_dwelling_median_sale_price": item.get(" rsdntl_prprty_prcs_yr_endd_30_jne_hss_mdn_sle_prce"),
            "house_median_sale_price": item.get(" rsdntl_prprty_prcs_yr_endd_30_jne_attchd_dwllngs_mdn_sle_prce"),
            "total_residential_building_value": item.get("value_tot_resial_building_aud000"),
            "new_other_residential_building_value": item.get(" value_new_oth_resial_building_aud000"),
            "new_houses_value": item.get(" value_new_houses_aud000"),
            "total_building_value": item.get(" value_tot_building_aud000"),
            "median_rent_weekly": item.get(" Median_rent_weekly")
        }
        standardized_data.append(standardized_item)
    return standardized_data


# Fetch data from each endpoint and standardize the keys
all_standardized_data = []
for endpoint in endpoints:
    response = requests.get(endpoint)
    if response.status_code == 200:
        print(f"Fetched data from {endpoint}")
        data = response.json()
        standardized_data = standardize_keys(data)
        all_standardized_data.extend(standardized_data)
    else:
        print(f"Failed to fetch data from {endpoint}")

# Create a single DataFrame from the combined standardized data
df = pd.DataFrame(all_standardized_data)
#print(df.head())

# # Save the DataFrame to a CSV file
# df.to_csv("data.csv", index=False)


# Ensure sa4_code is treated as an integer
df["sa4_code"] = pd.to_numeric(df["sa4_code"], errors="coerce").astype(pd.Int64Dtype())

# Group the data by area and year, and combine the columns using aggregation
result_df = df.groupby(["sa4_code", "year"]).agg({
    "sa4_name": "first",
    "attached_dwelling_median_sale_price": "first",
    "house_median_sale_price": "first",
    "total_residential_building_value": "first",
    "new_other_residential_building_value": "first",
    "new_houses_value": "first",
    "total_building_value": "first",
    "median_rent_weekly": "first"
}).reset_index()

# # Save the result to a CSV file
# result_df.to_csv("combined_data.csv", index=False)

print("Data has been saved to 'combined_data.csv'")


import pandas as pd
import geopandas as gpd
import folium
from folium import Choropleth, LayerControl

# # Load data
# df = pd.read_csv("combined_data.csv")


# Load the SA4 region boundaries
geojson_path = "/Users/HarutoKimura/Unimelb/semester1/cluster_and_computing/bda-c924-a2/frontend/SA4_2021_AUST_SHP_GDA2020/SA4_2021_AUST_GDA2020.shp"
gdf = gpd.read_file(geojson_path)

# Remove the non-finite entry
gdf = gdf[gdf['SA4_CODE21'] != 'ZZZ']

# Ensure 'sa4_code' is of integer type in both DataFrames
result_df["sa4_code"] = pd.to_numeric(df["sa4_code"], errors="coerce").astype(int)
gdf["SA4_CODE21"] = pd.to_numeric(gdf["SA4_CODE21"], errors="coerce").astype(int)

# Extract 'sa4_code' and 'geometry' from the GeoDataFrame and create a dictionary
geometry_dict = dict(zip(gdf["SA4_CODE21"], gdf["geometry"]))

# Add the geometry column to the DataFrame
result_df["geometry"] = result_df["sa4_code"].map(geometry_dict)
#print(df["geometry"])

# Export the geometry column to a CSV file
result_df[["sa4_code", "geometry"]].to_csv("geometry_data.csv", index=False)

print("Geometry data has been saved to 'geometry_data.csv'")


# Convert the DataFrame to a GeoDataFrame
gdf_combined = gpd.GeoDataFrame(result_df, geometry="geometry")

# Calculate the centroids of the geometries
gdf_combined["centroid"] = gdf_combined.geometry.centroid

# Extract latitude and longitude from the centroids
gdf_combined["latitude"] = gdf_combined.centroid.y
gdf_combined["longitude"] = gdf_combined.centroid.x

# Drop the 'geometry' and 'centroid' columns
df_with_lat_lon = gdf_combined.drop(columns=["geometry", "centroid"])

# Save to a CSV file
csv_path = "combined_data_with_lat_lon.csv"
df_with_lat_lon.to_csv(csv_path, index=False)

print(f"Combined data with latitude and longitude has been saved to '{csv_path}'")


import pandas as pd

# Load data with latitude and longitude
df = pd.read_csv("combined_data_with_lat_lon.csv")



# Load data from the CSV file
df = pd.read_csv("combined_data_with_lat_lon.csv")

# Group by 'sa4_code' and forward fill the missing 'sa4_name'
df['sa4_name'] = df.groupby('sa4_code')['sa4_name'].fillna(method='ffill')

# Group by 'sa4_code' and backward fill the missing 'sa4_name'
df['sa4_name'] = df.groupby('sa4_code')['sa4_name'].fillna(method='bfill')

# Save the updated DataFrame to a new CSV file
df.to_csv("combined_data_with_lat_lon_filled.csv", index=False)

print("Missing 'sa4_name' values have been filled and saved to 'combined_data_with_lat_lon_filled.csv'")


#visualise heatmap
import pandas as pd
import folium
from folium.plugins import HeatMap
from IPython.display import IFrame

# Create a base map centered on Australia
australia_center = [-25.2744, 133.7751]  # Latitude and longitude of the center of Australia
m = folium.Map(location=australia_center, zoom_start=4, tiles='OpenStreetMap')

# Define the column names for the housing prices
price_columns = [
    'attached_dwelling_median_sale_price',
    'house_median_sale_price',
    'total_residential_building_value',
    'new_other_residential_building_value',
    'new_houses_value',
    'total_building_value',
    'median_rent_weekly'
]

# Custom color gradient
gradient = {0.0: 'blue', 0.2: 'cyan', 0.4: 'lime', 0.6: 'yellow', 0.8: 'orange', 1.0: 'red'}


# Create a heatmap for each price column and year
for column in price_columns:
    for year in df['year'].unique():
        # Filter the data for the current year and column
        data = df[(df['year'] == year) & (df[column].notnull())]
        
        # Ensure there are no NaN values in latitude and longitude
        data = data.dropna(subset=['latitude', 'longitude'])
        
        # Extract the latitude, longitude, and price values
        heatmap_data = data[['latitude', 'longitude', column]].values.tolist()
        
        if not heatmap_data:
            continue  # Skip if there's no data for the heatmap
        
        # Create a heatmap layer
        heatmap = HeatMap(heatmap_data, name=f"{column} ({year})", radius=15, blur=15, max_zoom=18, gradient= gradient)
        
        # Add the heatmap layer to the map
        heatmap.add_to(m)

# Add the layer control to the map
folium.LayerControl(position='topright', collapsed=False).add_to(m)

# Save the map to an HTML file
map_filename = "output/heatmap.html"
m.save(map_filename)

print(f"Heatmap has been saved to '{map_filename}'")

# Display the map in a Jupyter notebook?? #<IPython.lib.display.IFrame at 0x15b74a980>
display(IFrame(map_filename, width=800, height=600))



#generate line graph (x: year, y: price, line: sa4_name)
import pandas as pd
import matplotlib.pyplot as plt
import mpld3
import seaborn as sns

# Load data from the CSV file
df = pd.read_csv("combined_data_with_lat_lon.csv")

# Function to plot housing prices for a selected price column and save as HTML
def plot_housing_prices(price_column, output_html, top_n):
    # Fill missing sa4_name values
    df['sa4_name'].fillna(method='ffill', inplace=True)
    
    # Filter the data to keep only the rows where the selected price column is not null
    data = df.dropna(subset=[price_column])
    
    # Check if the filtered data is empty
    if data.empty:
        print(f"No data available for {price_column}")
        return
    
    # Pivot the data to get years as rows and sa4_names as columns
    pivot_df = data.pivot(index='year', columns='sa4_name', values=price_column)
    
    # Check if pivoted data is empty
    if pivot_df.empty:
        print(f"No data available for {price_column} after pivoting")
        return
    
    # Select the top N regions with the highest mean values
    top_regions = pivot_df.mean().sort_values(ascending=False).head(top_n).index
    pivot_df = pivot_df[top_regions]
    
    # Plot the line graph
    fig, ax = plt.subplots(figsize=(12, 8))
    color_palette = sns.color_palette("tab10", len(pivot_df.columns))

    for color, column in zip(color_palette, pivot_df.columns):
        ax.plot(pivot_df.index, pivot_df[column], label=column, linewidth=3, marker='o', markersize=4, color=color)
    
    # Add labels and title
    ax.set_xlabel('Year', fontsize=16, labelpad=20)
    ax.set_ylabel(price_column.replace('_', ' ').title(), fontsize=16, labelpad=30)
    ax.set_title(f'{price_column.replace("_", " ").title()} Over the Years', fontsize=20, pad=20)
    
    # Add legend only if there are labels
    if pivot_df.columns.size > 0:
        ax.legend(title='SA4 Region', bbox_to_anchor=(0.15, 0.96), loc='upper left', fontsize='medium', title_fontsize='large', bbox_transform=fig.transFigure)
    
    ax.grid(True)

    # Set the x-axis ticks to display only unique years
    ax.set_xticks(sorted(data['year'].unique()))
    ax.set_xticklabels(sorted(data['year'].unique(),key=int))

    # Save the plot as an HTML file using mpld3
    mpld3.save_html(fig, output_html)
    print(f"Plot has been saved to '{output_html}'")

#usage
plot_housing_prices('total_residential_building_value', 'output/total_residential_building_value_plot.html', top_n=10) #okay
plot_housing_prices('attached_dwelling_median_sale_price', 'output/attached_dwelling_median_sale_price_plot.html', top_n=10) #okay
plot_housing_prices('house_median_sale_price', 'output/house_median_sale_price_plot.html', top_n=10) #okay
plot_housing_prices('new_other_residential_building_value', 'output/new_other_residential_building_value_plot.html', top_n=10) #okay
plot_housing_prices('new_houses_value', 'output/new_houses_value_plot.html', top_n=10) #okay
plot_housing_prices('total_building_value', 'output/total_building_value_plot.html', top_n=10) #okay
plot_housing_prices('median_rent_weekly', 'output/median_rent_weekly_plot.html', top_n=10) #okay

