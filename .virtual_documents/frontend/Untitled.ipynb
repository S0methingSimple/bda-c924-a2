import requests
import pandas as pd
import json
import config
import warnings
warnings.filterwarnings("ignore")

from elasticsearch import Elasticsearch





def divider(n=1):
    for i in range(n):
        print("-" * 20)





user_name, pw = config.ES_USERNAME, config.ES_PASSWORD


# Connect to Elasticsearch
es = Elasticsearch("https://127.0.0.1:9200", basic_auth=(user_name, pw), verify_certs=False)

# check the index list
indices = es.cat.indices(index='*', h='index', format='json')


for index in indices:
    print(index)


# loading all the data
raw_data = {}

def contain_region(vars_: list):
    for var in vars_:
        var = var.lower()
        if "lga" in var or "sa4" in var:
            return True
    return False

# query = {
#     "query": {
#         "match_all": {}
#     }
# }
for item in indices:
    query = {
        "query": {
            "match_all": {}
        }
    }
    index_ = item['index']
    print(f"retrieving index {index_}")
    # Grab the exact size of the dataset, elasticSearch default only returns 10
    temp_response = es.search(index=index_, body=query)
    
    # only grabbing sudo data
    try:
        vars_ = temp_response['hits']['hits'][0]['_source']
    except IndexError:
        print("no data in the index, aborting...")
        divider()
        continue
    if not contain_region(vars_): 
        print("not an sudo index, aborting...")
        divider()
        continue
    
    index_size = temp_response['hits']['total']['value']

    # Retrieve the full dataset
    full_response = es.search(
        index=index_, 
        body=query,
        size=index_size
    )

    raw_data[index_] = full_response['hits']['hits']

    divider()


assert len(raw_data) == 7, "There should be 7 SUDO datasets"





raw_data.keys()


all_sudo_data = {}

for index, raw_data_ in raw_data.items():
    raw_data_ = [item['_source'] for item in raw_data_]
    all_sudo_data[index] = raw_data_

df_sudo_data = {}
for index, data_ in all_sudo_data.items():
    df_sudo_data[index] = pd.DataFrame(data_)


# decode col names:
var_definitions = {}

data_definitions = get_ipython().getoutput('ls ./data_definition')

for data_definition in data_definitions:
    with open('./data_definition/' + data_definition, 'r') as f:
        temp_file = json.load(f)
        vars_ = temp_file['selectedAttributes']
        for var_ in vars_:
            var_definitions[var_['name']] = var_['title']


# Renaming cols to meaningful names
for index, df_ in df_sudo_data.items():
    columns = df_.columns.to_list()
    columns = [var_definitions[item.strip()] for item in columns]
    df_.columns = columns


# Check if year and SA4 code is in the columns, which are the two key variables

for index, df_ in df_sudo_data.items():
    print(index)
    print(f"Year in columns: {'Year' in df_.columns}")
    print(f"SA4 Code in columns: {'SA4 Code' in df_.columns}")
    divider()


# making up year or SA4 code, because they are used as index to join the dataframes
df_sudo_data['2021_lga_settlement_reports_permanent_settlers_by_migration_stream']['Year'] = 2020
df_sudo_data['building_approvals2011-2020'].rename(columns={'SA4 Code 2011': 'SA4 Code'}, inplace=True)
df_sudo_data['as4_median_housing_price2010_2014'].rename(columns={'SA4 Code 2011': 'SA4 Code'}, inplace=True)


df_sudo_data['as4_median_housing_price2010_2014'].columns


df_sudo_data['economy_and_industry_2014-2019'].columns


for index, df_ in df_sudo_data.items():
    print(index)
    print(df_['Year'].value_counts(dropna=False))


# Concatenate all the dataframes along the rows
# have to leave 2021 lga settlement data here because it's using a different partitioning system than the others: LGA

dfs = [df.set_index(['Year', 'SA4 Code']) for index, df in df_sudo_data.items() if index != '2021_lga_settlement_reports_permanent_settlers_by_migration_stream']
df_sudo = pd.concat(dfs, axis=0, join='outer')

df_sudo = df_sudo.sort_index(level=['Year', 'SA4 Code'])


df_sudo.shape


df_sudo.columns


df_sudo.reset_index(inplace=True)


df_sudo['State and Territory Id'] = df_sudo['SA4 Code'].apply(lambda x: x//100)


# Deleting columns with duplicate information such as complements
df_sudo = df_sudo.drop([
    'Household Stress - Census Households where rent payments are less than 30% of household income (%)',
    'Household Stress - Census Households where mortgage repayments are less than 30% of household income (%)',
    'Housing Suitability - Occupied private dwellings - Census Dwellings with bedrooms spare (no.)',
    'Housing Suitability - Occupied private dwellings - Census Dwellings with no bedrooms needed or spare (no.)',
    'Value of Total Buildings ($000)',
    'Value of Non-Residential Building ($000)',
    'Housing Suitability - Occupied private dwellings - Census Dwellings with bedrooms spare (no.)',
    'Housing Suitability - Occupied private dwellings - Census Dwellings with no bedrooms needed or spare (no.)',
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $1-$499 per week (%)',
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $500-$999 per week (%)',
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $1000-$1999 per week (%)', 
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $2000-$2999 per week (%)',
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $3000 or more per week (%)',  
],
    axis=1,
    errors='ignore'
)


df_sudo = df_sudo.replace('null', pd.NA)


# Converting data types
# Seems unnecessary once I converted 'null' to pd.NA, because null is causing pandas to render a lot of the 
# variables as object
# will leave it here incase I need it 

df_sudo = df_sudo.astype(
    {
        'Housing Suitability - Occupied private dwellings - Census Dwellings with extra bedrooms needed (no.)': 'int64', 
        'Residential Property Prices - Year ended 30 June Houses - median sale price ($)': 'int64',
        'Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)': 'int64',
        'Building Approvals - Year ended 30 June Value of residential building ($m)': 'int64',
        'Building Approvals - Year ended 30 June Private sector dwellings excluding houses (no.)': 'int64',
        'Residential Property Prices - Year ended 30 June Attached Dwellings - median sale price ($)': 'int64',
        'Building Approvals - Year ended 30 June Total private sector dwelling units (no.)': 'int64',
        'Building Approvals - Year ended 30 June Total value of private sector dwelling units ($m)': 'int64',
        'Building Approvals - Year ended 30 June Private sector houses (no.)': 'int64',
        'Building Approvals - Year ended 30 June Total dwelling units (no.)': 'int64',
        'Building Approvals - Year ended 30 June Value of private sector houses ($m)': 'int64',
        'Building Approvals - Year ended 30 June Value of private sector dwellings excluding houses ($m)': 'int64',
        'Residential Property Prices - Year ended 30 June Houses - number of transfers (no.)': 'int64',
        'Estimates of Personal Income - Year ended 30 June Total income (excl. Government pensions and allowances) - Gini coefficient': 'float64',
        'Estimates of Personal Income - Year ended 30 June Mean employee income ($)': 'int64',
        'Gross Capital Gains reported by taxpayers - Year ended 30 June Gross Capital Gains reported by taxpayers - Mean ($)': 'int64',
        'Estimates of Personal Income - Year ended 30 June Mean investment income ($)': 'int64',
        'Estimates of Personal Income - Year ended 30 June Median employee income ($)': 'int64',
        'Gross Capital Gains reported by taxpayers - Year ended 30 June Gross Capital Gains reported by taxpayers  - Median ($)': 'int64',
        'Estimates of Personal Income - Year ended 30 June Median investment income ($)': 'int64',
        'Residential Property Median Attached Dwelling Sale Price ($)': 'int64',
        'Residential Property Number of House Transfers': 'int64',
        'Residential Property Number of Attached Dwelling Transfers': 'int64',
        'Residential Property Median House Sale Price ($)': 'int64',
        'State and Territory Id': 'category'
    }, 
    errors='ignore'
)


df_sudo.dtypes


# Combine economy_and_industry_2014-2019's house price with 
# economy_and_industry_2014-2019's house price

## House
df_sudo['House Median Sales Price'] = \
    df_sudo['Residential Property Prices - Year ended 30 June Houses - median sale price ($)'].combine_first(
        df_sudo['Residential Property Median House Sale Price ($)']
    )
df_sudo['House No of Transactions'] = \
    df_sudo['Residential Property Prices - Year ended 30 June Houses - number of transfers (no.)'].combine_first(
        df_sudo['Residential Property Number of House Transfers']
    )

## Attached dwellings
df_sudo['Attached dwellings Median Sales Price'] = \
    df_sudo['Residential Property Prices - Year ended 30 June Attached Dwellings - median sale price ($)']\
    .combine_first(
        df_sudo['Residential Property Median Attached Dwelling Sale Price ($)']
    )
df_sudo['Attached dwellings No of Transactions'] = \
    df_sudo['Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)']\
    .combine_first(
        df_sudo['Residential Property Number of Attached Dwelling Transfers']
    )

df_sudo = df_sudo.drop(
    [
        'Residential Property Prices - Year ended 30 June Houses - median sale price ($)',
        'Residential Property Median House Sale Price ($)',
        'Residential Property Prices - Year ended 30 June Houses - number of transfers (no.)',
        'Residential Property Number of House Transfers',
        'Residential Property Prices - Year ended 30 June Attached Dwellings - median sale price ($)',
        'Residential Property Median Attached Dwelling Sale Price ($)',
        'Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)',
        'Residential Property Number of Attached Dwelling Transfers'
    ],
    axis=1
)


df_sudo.shape








import matplotlib.pyplot as plt
import seaborn as sns


STATE_TERRITORY_ID = {
    1: 'NSW',
    2: 'VIC',
    3: 'QLD',
    4: 'SA',
    5: 'WA',
    6: 'TAS',
    7: 'NT', # Northern Territory
    8: 'ACT' # Australian Capital Territory
}


df_sudo.columns


df_sudo['Attached dwellings No of Transactions'] = pd.to_numeric(df_sudo['Attached dwellings No of Transactions'], errors='coerce')

# Group by 'Year' and 'State and Territory Id' and sum the transactions
grouped_df = df_sudo.groupby(['Year', 'State and Territory Id']).agg({
    'Attached dwellings No of Transactions': 'sum'
})


# df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})

grouped_df = df_sudo.groupby(['Year', 'State and Territory Id']).\
    agg({'Attached dwellings No of Transactions': 'sum'})


df_sudo[df_sudo.apply(lambda x: x['Year'] == 2013 and x['State and Territory Id'] == 1, axis=1)]['Attached dwellings No of Transactions']


for row in grouped_df.iterrows():
    print(row)


plt.figure(figsize=(10, 6))
ax = sns.lineplot(
    data=grouped_df, 
    x='Year', 
    y='Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)',
    hue='SA4 Code'
)


grouped_df[grouped_df != 0]


test = df_sudo[df_sudo.apply(lambda x: x['Year']==2014 and x['State and Territory Id']==1, axis=1)]\
[[
    'Year',
    'SA4 Code',
    'Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)',
]]


test['Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)'].value_counts()


test['Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)'].sum()


plt.figure(figsize=(10, 6))
ax = sns.lineplot(
    data=df_sudo, 
    x='Year', 
    y='Household Stress - Census Households with rent payments greater than or equal to 30% of household income (%)',
    hue='SA4 Code'
)






























