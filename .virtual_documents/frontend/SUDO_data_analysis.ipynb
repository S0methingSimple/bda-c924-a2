import re
import requests
import pandas as pd
import json
import config
import warnings
warnings.filterwarnings("ignore")

from elasticsearch import Elasticsearch





def divider(n=1):
    for i in range(n):
        print("-" * 20)

def full(df):
    """
    display the full pandas dataframe
    """
    with pd.option_context("display.max_rows", None):
        display(df)





user_name, pw = config.ES_USERNAME, config.ES_PASSWORD


# Connect to Elasticsearch
es = Elasticsearch("https://127.0.0.1:9200", basic_auth=(user_name, pw), verify_certs=False)

# check the index list
indices = es.cat.indices(index='*', h='index', format='json')


for index in indices:
    print(index)


# loading all the data
raw_data = {}

def contain_region(vars_: list):
    for var in vars_:
        var = var.lower()
        if "lga" in var or "sa4" in var:
            return True
    return False

for item in indices:
    query = {
        "query": {
            "match_all": {}
        }
    }
    index_ = item['index']
    print(f"retrieving index {index_}")
    # Grab the exact size of the dataset, elasticSearch default only returns 10
    temp_response = es.search(index=index_, body=query)
    
    # only grabbing sudo data
    try:
        vars_ = temp_response['hits']['hits'][0]['_source']
    except IndexError:
        print("no data in the index, aborting...")
        divider()
        continue
    if not contain_region(vars_): 
        print("not an sudo index, aborting...")
        divider()
        continue
    
    index_size = temp_response['hits']['total']['value']

    # Retrieve the full dataset
    full_response = es.search(
        index=index_, 
        body=query,
        size=index_size
    )

    raw_data[index_] = full_response['hits']['hits']

    divider()


assert len(raw_data) == 7, "There should be 7 SUDO datasets"





raw_data.keys()


all_sudo_data = {}

for index, raw_data_ in raw_data.items():
    raw_data_ = [item['_source'] for item in raw_data_]
    all_sudo_data[index] = raw_data_

df_sudo_data = {}
for index, data_ in all_sudo_data.items():
    df_sudo_data[index] = pd.DataFrame(data_)


df_sudo_data.keys()


# decode col names:
var_definitions = {}

data_definitions = get_ipython().getoutput('ls ./data_definition')

for data_definition in data_definitions:
    with open('./data_definition/' + data_definition, 'r') as f:
        temp_file = json.load(f)
        vars_ = temp_file['selectedAttributes']
        for var_ in vars_:
            var_definitions[var_['name']] = var_['title']


# Renaming cols to meaningful names
for index, df_ in df_sudo_data.items():
    columns = df_.columns.to_list()
    columns = [var_definitions[item.strip()] for item in columns]
    df_.columns = columns


# Check if year and SA4 code is in the columns, which are the two key variables

for index, df_ in df_sudo_data.items():
    print(index)
    print(f"Year in columns: {'Year' in df_.columns}")
    print(f"SA4 Code in columns: {'SA4 Code' in df_.columns}")
    divider()


# making up year or SA4 code, because they are used as index to join the dataframes
df_sudo_data['2021_lga_settlement_reports_permanent_settlers_by_migration_stream']['Year'] = 2020
df_sudo_data['building_approvals2011-2020'].rename(columns={'SA4 Code 2011': 'SA4 Code'}, inplace=True)
df_sudo_data['as4_median_housing_price2010_2014'].rename(columns={'SA4 Code 2011': 'SA4 Code'}, inplace=True)


df_sudo_data['as4_median_housing_price2010_2014'].columns


for index, df_ in df_sudo_data.items():
    print(index)
    print(df_['Year'].value_counts(dropna=False))


# Concatenate all the dataframes along the rows
# have to leave 2021 lga settlement data here because it's using a different partitioning system than the others: LGA

dfs = [df.set_index(['Year', 'SA4 Code']) for index, df in df_sudo_data.items() if index != '2021_lga_settlement_reports_permanent_settlers_by_migration_stream']
df_sudo = pd.concat(dfs, axis=0, join='outer')

df_sudo = df_sudo.sort_index(level=['Year', 'SA4 Code'])


df_sudo = df_sudo.groupby(level=[0, 1]).first()


df_sudo


df_sudo.shape


df_sudo.reset_index(inplace=True)


df_sudo['State and Territory Id'] = df_sudo['SA4 Code'].apply(lambda x: x//100)


# Deleting columns with duplicate information such as complements
df_sudo = df_sudo.drop([
    'Household Stress - Census Households where rent payments are less than 30% of household income (%)',
    'Household Stress - Census Households where mortgage repayments are less than 30% of household income (%)',
    'Housing Suitability - Occupied private dwellings - Census Dwellings with bedrooms spare (no.)',
    'Housing Suitability - Occupied private dwellings - Census Dwellings with no bedrooms needed or spare (no.)',
    'Value of Total Buildings ($000)',
    'Value of Non-Residential Building ($000)',
    'Housing Suitability - Occupied private dwellings - Census Dwellings with bedrooms spare (no.)',
    'Housing Suitability - Occupied private dwellings - Census Dwellings with no bedrooms needed or spare (no.)',
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $1-$499 per week (%)',
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $500-$999 per week (%)',
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $1000-$1999 per week (%)', 
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $2000-$2999 per week (%)',
    'Total Personal Income (Weekly) - Persons aged 15 years and over  - Census Persons earning $3000 or more per week (%)',  
],
    axis=1,
    errors='ignore'
)


df_sudo = df_sudo.replace('null', pd.NA)
df_sudo = df_sudo.replace('None', pd.NA)


# Combine economy_and_industry_2014-2019's house price with 
# economy_and_industry_2014-2019's house price

## House
df_sudo['House Median Sales Price'] = \
    df_sudo['Residential Property Prices - Year ended 30 June Houses - median sale price ($)'].combine_first(
        df_sudo['Residential Property Median House Sale Price ($)']
    )
df_sudo['House No of Transactions'] = \
    df_sudo['Residential Property Prices - Year ended 30 June Houses - number of transfers (no.)'].combine_first(
        df_sudo['Residential Property Number of House Transfers']
    )

## Attached dwellings
df_sudo['Attached dwellings Median Sales Price'] = \
    df_sudo['Residential Property Prices - Year ended 30 June Attached Dwellings - median sale price ($)']\
    .combine_first(
        df_sudo['Residential Property Median Attached Dwelling Sale Price ($)']
    )
df_sudo['Attached dwellings No of Transactions'] = \
    df_sudo['Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)']\
    .combine_first(
        df_sudo['Residential Property Number of Attached Dwelling Transfers']
    )

df_sudo = df_sudo.drop(
    [
        'Residential Property Prices - Year ended 30 June Houses - median sale price ($)',
        'Residential Property Median House Sale Price ($)',
        'Residential Property Prices - Year ended 30 June Houses - number of transfers (no.)',
        'Residential Property Number of House Transfers',
        'Residential Property Prices - Year ended 30 June Attached Dwellings - median sale price ($)',
        'Residential Property Median Attached Dwelling Sale Price ($)',
        'Residential Property Prices - Year ended 30 June Attached Dwellings - number of transfers (no.)',
        'Residential Property Number of Attached Dwelling Transfers'
    ],
    axis=1
)


# Converting data types
# Seems unnecessary once I converted 'null' to pd.NA, because null is causing pandas to render a lot of the 
# variables as object
# will leave it here incase I need it 

numeric_columns = [
    'Housing Suitability - Occupied private dwellings - Census Dwellings with extra bedrooms needed (no.)', 
    'Building Approvals - Year ended 30 June Value of residential building ($m)',
    'Building Approvals - Year ended 30 June Private sector dwellings excluding houses (no.)',
    'Building Approvals - Year ended 30 June Total private sector dwelling units (no.)',
    'Building Approvals - Year ended 30 June Total value of private sector dwelling units ($m)',
    'Building Approvals - Year ended 30 June Private sector houses (no.)',
    'Building Approvals - Year ended 30 June Total dwelling units (no.)',
    'Building Approvals - Year ended 30 June Value of private sector houses ($m)',
    'Building Approvals - Year ended 30 June Value of private sector dwellings excluding houses ($m)',
    'Estimates of Personal Income - Year ended 30 June Total income (excl. Government pensions and allowances) - Gini coefficient',
    'Estimates of Personal Income - Year ended 30 June Mean employee income ($)',
    'Gross Capital Gains reported by taxpayers - Year ended 30 June Gross Capital Gains reported by taxpayers - Mean ($)',
    'Estimates of Personal Income - Year ended 30 June Mean investment income ($)',
    'Estimates of Personal Income - Year ended 30 June Median employee income ($)',
    'Gross Capital Gains reported by taxpayers - Year ended 30 June Gross Capital Gains reported by taxpayers  - Median ($)',
    'Estimates of Personal Income - Year ended 30 June Median investment income ($)',
    'House Median Sales Price',
    'House No of Transactions',
    'Attached dwellings Median Sales Price',
    'Attached dwellings No of Transactions'
]

for column in numeric_columns:
    df_sudo[column] = pd.to_numeric(df_sudo[column], errors='coerce')

# df_sudo = df_sudo.astype(
#     {
#         'Housing Suitability - Occupied private dwellings - Census Dwellings with extra bedrooms needed (no.)': 'int64', 
#         'Building Approvals - Year ended 30 June Value of residential building ($m)': 'int64',
#         'Building Approvals - Year ended 30 June Private sector dwellings excluding houses (no.)': 'int64',
#         'Building Approvals - Year ended 30 June Total private sector dwelling units (no.)': 'int64',
#         'Building Approvals - Year ended 30 June Total value of private sector dwelling units ($m)': 'int64',
#         'Building Approvals - Year ended 30 June Private sector houses (no.)': 'int64',
#         'Building Approvals - Year ended 30 June Total dwelling units (no.)': 'int64',
#         'Building Approvals - Year ended 30 June Value of private sector houses ($m)': 'int64',
#         'Building Approvals - Year ended 30 June Value of private sector dwellings excluding houses ($m)': 'int64',
#         'Estimates of Personal Income - Year ended 30 June Total income (excl. Government pensions and allowances) - Gini coefficient': 'float64',
#         'Estimates of Personal Income - Year ended 30 June Mean employee income ($)': 'int64',
#         'Gross Capital Gains reported by taxpayers - Year ended 30 June Gross Capital Gains reported by taxpayers - Mean ($)': 'int64',
#         'Estimates of Personal Income - Year ended 30 June Mean investment income ($)': 'int64',
#         'Estimates of Personal Income - Year ended 30 June Median employee income ($)': 'int64',
#         'Gross Capital Gains reported by taxpayers - Year ended 30 June Gross Capital Gains reported by taxpayers  - Median ($)': 'int64',
#         'Estimates of Personal Income - Year ended 30 June Median investment income ($)': 'int64',
#         'State and Territory Id': 'category',
#         'House Median Sales Price': 'int64',
#         'House No of Transactions': 'int64',
#         'Attached dwellings Median Sales Price': 'int64',
#         'Attached dwellings No of Transactions': 'int64'
#     }, 
#     errors='ignore'
# )



df_sudo.dtypes


df_sudo.shape








import matplotlib.pyplot as plt
import seaborn as sns

import plotly.express as px


STATE_TERRITORY_ID = {
    1: 'NSW',
    2: 'VIC',
    3: 'QLD',
    4: 'SA',
    5: 'WA',
    6: 'TAS',
    7: 'NT', # Northern Territory
    8: 'ACT' # Australian Capital Territory
}


MEL_SYD_SA4_NAME = [
    'Sydney - Outer South West',
    'Sydney - Inner West',
    'Sydney - Inner South West',
    'Sydney - Eastern Suburbs',
    'Sydney - City and Inner South',
    'Sydney - Blacktown',
    'Sydney - Baulkham Hills and Hawkesbury'
    'Sydney - Northern Beaches', 
    'Sydney - North Sydney and Hornsby',
    'Sydney - Outer West and Blue Mountains'
    'Sydney - Parramatta', 
    'Sydney - Sutherland', 
    'Sydney - South West',
    'Sydney - Ryde'
    'Melbourne - West', 
    'Melbourne - South East', 
    'Melbourne - Outer East',
    'Melbourne - North West',
    'Melbourne - North East',
    'Melbourne - Inner South', 
    'Melbourne - Inner East',
    'Melbourne - Inner',
]

MEL_SYD_SA4_NAME_TO_CODE = {}
MEL_SYD_SA4_CODE_TO_NAME = {}


for sa4 in MEL_SYD_SA4_NAME:
    try: 
        SA4_code = df_sudo[df_sudo['SA4 Name'] == sa4]['SA4 Code'].mode()[0]
        MEL_SYD_SA4_NAME_TO_CODE[sa4] = SA4_code
        MEL_SYD_SA4_CODE_TO_NAME[SA4_code] = sa4
    except: 
        continue


MEL_SYD_SA4_NAME_TO_CODE


df_sudo.to_csv('df_sudo.csv')


df_sudo_major = df_sudo[df_sudo.apply(lambda x: x['SA4 Code'] in MEL_SYD_SA4_NAME_TO_CODE.values(), axis=1)]


df_sudo_major['SA4 Name'] = df_sudo_major['SA4 Code'].apply(lambda x: MEL_SYD_SA4_CODE_TO_NAME[x])


df_sudo_major['SA4 Code'] = df_sudo_major['SA4 Code'].astype('category')


df_sudo_major['City'] = df_sudo_major['SA4 Name'].apply(
    lambda x: re.match('^([A-Z][a-z]+) - [A-Za-z ]+$', x)[1]
)


# df_sudo_major.to_csv('df_sudo_major.csv')


df_sudo_major.columns


# df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})

grouped_df = df_sudo.groupby(['Year', 'State and Territory Id']).\
    agg({
        'Attached dwellings No of Transactions': 'sum'
    })


# df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})

grouped_df = df_sudo.groupby(['Year', 'State and Territory Id']).\
    agg({
        'Attached dwellings No of Transactions': 'sum',
        'Household Stress - Census Households with rent payments greater than or equal to 30% of household income (%)': 'sum'
    })





def get_IRR(start: int, end: int, var: str, df=df_sudo_major):
    filtered_df = df[df['Year'].isin([start, end])]
    pivot_df = filtered_df.pivot(index='SA4 Name', columns='Year', values=var)
    pivot_df['Rate of Change'] = ((pivot_df[end] / pivot_df[start]) ** (1/(end-start)) - 1) * 100

    ax = sns.barplot(
        data=pivot_df,
        x='Rate of Change',
        y='SA4 Name',
        hue='Rate of Change'
    )

    # Annotate each bar with the value from the data
    for p in ax.patches:  # loop through each bar
        width = p.get_width()  # get the width of the bar
        ax.text(
            width + 0.05,  # x-coordinate position of text
            p.get_y() + p.get_height() / 2,  # y-coordinate position of text
            '{:1.2f}'.format(width),  # text label
            va='center'  # center alignment
        )
    
    return pivot_df








pivot_df = get_IRR(2011, 2021, 'Median rent weekly')


ax = sns.lineplot(
    data=df_sudo_major,
    x="Year",
    y='Median rent weekly',
    hue='SA4 Name',
    style='City',
    palette='flare'
)
sns.move_legend(ax, "center right", bbox_to_anchor=(1.65, 0.5))





# Assuming df_sudo is already loaded and ready to use
fig = px.line(df_sudo_major, x="Year", y='Median rent weekly', color='SA4 Name', 
              title='Median Weekly Rent by SA4 Code Over Years')

# Moving the legend to the right of the plot and making it interactive
fig.update_layout(
    legend=dict(
        title='SA4 Name',
        orientation="v",
        yanchor="middle",
        y=0.5,
        xanchor="right",
        x=1.05
    )
)

fig.show()





pivot_df = get_IRR(2011, 2021, 'Median mortgage repayment monthly')


ax = sns.lineplot(
    data=df_sudo_major,
    x="Year",
    y='Median mortgage repayment monthly',
    hue='SA4 Name',
    palette='flare',
    style='City'
)
sns.move_legend(ax, "center right", bbox_to_anchor=(1.65, 0.5))


get_ipython().getoutput("pwd")

































